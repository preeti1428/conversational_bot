# -*- coding: utf-8 -*-
"""encode-decode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HNyj_amRsXPB-8lnfNUdNBc91aM94_Ab
"""

import tensorflow as tf

import tensorflow_datasets as tfds
import os

DIRECTORY_URL = #load data
FILE_NAMES = ##load data

for name in FILE_NAMES:
  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)
  
parent_dir = os.path.dirname(text_dir)

parent_dir

##texts into large dataset

def labeler(example, index):
  return example, tf.cast(index, tf.int64)  

labeled_data_sets = []

for i, file_name in enumerate(FILE_NAMES):
  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))
  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))
  labeled_data_sets.append(labeled_dataset)

BUFFER_SIZE = 50000
BATCH_SIZE = 64
TAKE_SIZE = 5000

all_labeled_data = labeled_data_sets[0]
for labeled_dataset in labeled_data_sets[1:]:
  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)
  
all_labeled_data = all_labeled_data.shuffle(
    BUFFER_SIZE, reshuffle_each_iteration=False)

##text lines as numbers
##vocab of own

tokenizer = tfds.features.text.Tokenizer()

vocabulary_set = set()
for text_tensor, _ in all_labeled_data:
  some_tokens = tokenizer.tokenize(text_tensor.numpy())
  vocabulary_set.update(some_tokens)

vocab_size = len(vocabulary_set)
vocab_size

crypter = tfds.features.text.TokenTextEncoder(vocabulary_set)

def crypt(text_tensor, label):
  crypted_text = crypter.crypt(text_tensor.numpy())
  return crypted_text, label

##map all texts with labels
def crypt_map_fn(text, label):
  # py_func doesn't set the shape of the returned tensors.
  crypted_text, label = tf.py_function(crypt, 
                                       inp=[text, label], 
                                       Tout=(tf.int64, tf.int64))

  # `tf.data.Datasets` work best if all components have a shape set
  #  so set the shapes manually: 
  crypted_text.set_shape([None])
  label.set_shape([])

  return crypted_text, label


all_crypted_data = all_labeled_data.map(crypt_map_fn)

##splitting datasets 

train_data = all_crypted_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)
train_data = train_data.padded_batch(BATCH_SIZE)

test_data = all_crypted_data.take(TAKE_SIZE)
test_data = test_data.padded_batch(BATCH_SIZE)

###building model
##ENCODER

from keras.layers import LSTM

input_seq_encoder = Input(shape = (None, ),
                          name = "encoder_input")     # (BATCH_SIZE, sentence_length, 1) 

embed_dim = 200
embedded_seq_encoder = Embedding(input_dim = vocab_size, 
                                 output_dim = embed_dim)(input_seq_encoder)

encoder_lstm = LSTM(units = 256,             
                    activation = 'relu',
                    return_sequences = False,
                    return_state = True,
                    name = "encoder_LSTM")

_, last_hidden_encoder, last_cell_encoder = encoder_lstm(embedded_seq_encoder)

#define decoder
input_seq_decoder = Input(shape = (None, 1),
                          name = "decoder_input")     # (batch_size, sentence_length, 1)

decoder_lstm = LSTM(units = 256,                          
                    activation = 'relu',
                    return_sequences = True,
                    return_state = True,
                    name = "decoder_LSTM")

all_hidden_decoder, _, _ = decoder_lstm(input_seq_decoder, 
                                        initial_state = [last_hidden_encoder, last_cell_encoder])

decoder_dense = Dense(ed_french_vocab_size,   # NOT TIMEDISTRIBUTED (NOT RECURSIVE)
                      activation = 'softmax',
                      name = "decoder_dense")
logits = decoder_dense(all_hidden_decoder)


# 3. Define Model
final_rnn_model = Model(input = [input_seq_encoder, input_seq_decoder],
                        output = logits)

final_rnn_model.compile(loss = sparse_categorical_crossentropy,
                        optimizer = Adam(lr = 0.002),
                        metrics = ['accuracy'])

# 4. Fit the Model
final_rnn_model.fit([english_input, decoder_french_input],
                    decoder_french_target,
                    batch_size = 1024,
                    epochs = 16,
                    validation_split = 0.2)

model.fit(train_data, epochs=3, validation_data=test_data)

eval_loss, eval_acc = model.evaluate(test_data)

print('\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))

#model = tf.keras.Sequential() 
#model.add(tf.keras.layers.Embedding(vocab_size, 64)) 
#model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))  

#for units in [64, 64]:   
 # model.add(tf.keras.layers.Dense(units, activation='relu'))  # Output layer. The first argument is the number of labels. 
 # model.add(tf.keras.layers.Dense(3))  
 # model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])

from math import log
from numpy import array
from numpy import argmax

# beam search
def beam_search_decoder(test_data, k):
	sequences = [[list(), 0.0]]
	# walk over each step in sequence
	for row in test_data:
		all_candidates = list()
		# expand each current candidate
		for i in range(len(sequences)):
			seq, score = sequences[i]
			for j in range(len(row)):
				candidate = [seq + [j], score - log(row[j])]
				all_candidates.append(candidate)
		# order all candidates by score
		ordered = sorted(all_candidates, key=lambda tup:tup[1])
		# select k best , here '3'
		sequences = ordered[:k]
	return sequences

data = array(data) 
# decode sequence
result = beam_search_decoder(data, 3)
# print result
for seq in result:
	print(seq)